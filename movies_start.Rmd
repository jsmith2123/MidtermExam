---
title: "movies_start"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Part 1: Predicting Ratings from Tags and Genre

## 1.1 Load and Prepare Data

First, we set our working directory and load the three main data files.

> **Note**: You must change the `setwd()` path to the directory where you have saved the data files on your computer.

```{r load-data, eval=TRUE}
# Set this to your local directory
setwd("/Users/jinwonsohn/Desktop/vcode/MIDTERM/DATA/")

# These are three main files containing ratings and movie attributes
names <- read.csv("movies.csv", header = TRUE)      # Links movie ID to titles and genres
data <- read.csv("scores.csv", header = TRUE)       # User ratings for movies
attributes <- read.csv("attributes.csv", header = TRUE) # User-generated tags (characteristics)
```

## 1.2 Assemble Predictor (X) and Response (Y) Matrices

The code below processes the raw data to create a response vector `Y` (the average rating for each movie) and a predictor matrix `X` (a matrix of 0/1 dummy variables for genres and tags).

### Create the Response Vector Y

We'll calculate the average rating for each movie across all users who rated it.

```{r create-y}
library(Matrix)

# Calculate the mean rating for each unique movieId
average_rating <- tapply(data$rating, data$movieId, mean)

# Convert this into a column matrix, which will be our response Y
Y <- cbind(average_rating)
rownames(Y) <- names(table(data$movieId))
```

### Create the Predictor Matrix X

This is a multi-step process where we convert text-based genres and tags into a numerical matrix.

First, we create 0/1 dummy variables for genres from the `movies.csv` file.

```{r create-x-genre}
# Inspect the data
head(names)

# Split the 'genres' string (e.g., "Action|Adventure|Sci-Fi") into a list
spl <- lapply(strsplit(names$genres, "\\|"), trimws)

# Find all unique genres across all movies
all_genres <- sort(unique(unlist(spl)))

# Create a matrix where each column is a genre and each row is a movie.
# The value is 1 if the movie has that genre, 0 otherwise.
X_genre <- sapply(all_genres, function(g) {
  as.integer(vapply(spl, function(v) g %in% v, logical(1)))
})
  
dimnames(X_genre) <- list(seq_along(names$genres), all_genres)
rownames(X_genre) <- names$movieId
```

Next, we do the same for the movie tags/attributes from `attributes.csv`.

```{r create-x-attributes}
# Inspect the data
head(attributes)

# Group tags by movieId
attr <- split(x = attributes$tag, f = attributes$movieId)

# Helper function to create a 0/1 incidence matrix from a list of attributes
make_incidence <- function(attrs) {
  attrs <- lapply(attrs, unique)
  all_attr <- sort(unique(unlist(attrs)))
  item_ids <- if (is.null(names(attrs))) as.character(seq_along(attrs)) else names(attrs)
  
  M <- matrix(0L, nrow = length(attrs), ncol = length(all_attr),
              dimnames = list(item_ids, all_attr))

  for (i in seq_along(attrs)) {
    if (length(attrs[[i]])) M[i, attrs[[i]]] <- 1L
  }
  M
}

A <- make_incidence(attr)

# To reduce noise, we only keep attributes that appeared in at least 10 movies
sums <- apply(A, 2, sum)
A <- A[, sums > 10]
```

### Combine and Align X and Y

Finally, we merge the genre and attribute matrices and ensure that the movies in `X` and `Y` are perfectly aligned.

```{r combine-and-align}
# Match the attribute data (A) to the genre data (X_genre)
keep <- rownames(X_genre) %in% rownames(A)
X_genre <- X_genre[keep, ]
m <- match(rownames(X_genre), rownames(A))

# Create the final attribute matrix, aligned with the genre matrix
X_attr <- A[m, ]

# Combine attributes and genres into our final predictor matrix X
X <- cbind(X_attr, X_genre)
cat("Dimensions of final X matrix:", dim(X), "\n")

# Finally, make sure the Y vector matches the X matrix perfectly
keep <- rownames(X) %in% rownames(Y)
X <- X[keep, ]
m <- match(rownames(X), rownames(Y))
Y <- Y[m, ]

cat("Final number of movies with complete data:", nrow(X))
```

***

## 1.3 LASSO Analysis

Now we can use the **LASSO (Least Absolute Shrinkage and Selection Operator)** to predict movie ratings. LASSO is a regression technique that performs feature selection, automatically identifying the most important genres and tags for predicting ratings.

### Question 1.1: Fit the LASSO Model

We'll use the `gamlr` package to fit the model.

```{r run-lasso}
library(gamlr)

Y <- as.numeric(Y)

# Fit the LASSO model across a path of penalty values (lambda)
lasso <- gamlr(X, Y)
plot(lasso)
```

The plot above shows the **coefficient path**. Each line represents a predictor (a genre or tag). As the penalty (log lambda) increases to the right, more coefficients are "shrunk" to exactly zero, effectively removing them from the model.

Let's see which predictors have non-zero coefficients for a particular penalty value.

```{r lasso-coefs}
beta <- coef(lasso)
beta <- beta[-1, ] # Remove the intercept
beta[beta != 0]
```

### Question 1.2: Cross-Validation to Find the Optimal Model

To find the best penalty level (`lambda`), we use **cross-validation**. This process helps select a model that generalizes well to new, unseen data.

```{r run-cv-lasso}
# Run 10-fold cross-validation
cv.lasso <- cv.gamlr(X, Y)
plot(cv.lasso)
```

The plot shows the cross-validated mean squared error (MSE) across different log-lambda values. The vertical dashed lines indicate the lambda that gives the minimum MSE (`lambda.min`) and the lambda that gives an error within one standard error of the minimum (`lambda.1se`), which is a more parsimonious choice.

We can extract the coefficients from the model selected by cross-validation. These are the tags and genres that are most predictive of a movie's rating.

```{r cv-lasso-coefs}
beta_cv <- coef(cv.lasso)
beta_cv <- beta_cv[-1, ] # Remove intercept
beta_cv[beta_cv != 0]
```

***

# Part 2: Building a Recommender System

Here, we will use **association rule mining** to find relationships between movies. The goal is to generate rules like `{Forrest Gump (1994)} => {Pulp Fiction (1994)}`, which means that users who liked *Forrest Gump* also tended to like *Pulp Fiction*.

## 2.1 Prepare Transaction Data

First, we transform the ratings data into a "transaction" format, where each transaction is a list of movies that a user rated highly (4 stars or more).

```{r prep-transactions}
# We consider a rating of 4 or higher as a "like"
data$binary_rating <- data$rating >= 4
subset <- data[data$binary_rating == 1, ]

# Match movie IDs to actual movie titles for readability
m <- match(subset$movieId, names$movieId)
subset$movieId <- names[m, 2]

# Create a list where each element is a vector of movies liked by a single user
movielists <- split(x = subset$movieId, f = subset$userId)
movielists <- lapply(movielists, unique) # Remove duplicate likes per user

# Convert the list into a 'transactions' object for the arules package
library(arules)
movietrans <- as(movielists, "transactions")
summary(movietrans)
```

## 2.2 Mine for Association Rules

We will use the **Apriori algorithm** to find frequent itemsets and generate rules. We focus on three key metrics:
* **Support**: The fraction of users who liked the movie(s) in the rule.
* **Confidence**: The probability of liking the movie on the right-hand side, given that the user liked the movie(s) on the left-hand side.
* **Lift**: How much more likely a user is to like the right-hand movie, given they liked the left-hand one, compared to the baseline probability. A lift > 1 indicates a positive association.

### Find High-Confidence, High-Support Rules

Your task is to find rules with at least **10% support** and **95% confidence**. Try other values by yourselves!

```{r apriori-task1}
# Fill in the blanks for support and confidence below
set.seed(123)
rules <- apriori(movietrans,
                 parameter = list(support = 0.1, confidence = 0.95, maxlen = 2))

head(rules)
```

### Find High-Lift Rules

Now, find rules with at least **1% support** and **50% confidence**, and then filter for rules with a **lift greater than 10**.

```{r apriori-task2}
# Fill in the blanks for support and confidence below
rules2 <- apriori(movietrans,
                  parameter = list(support = 0.01, confidence = 0.5, maxlen = 2))

# Subset the rules to keep only those with high lift
subrules <- subset(rules2, subset = lift > 10)
head(sort(subrules, by = "lift"))
```

***

# Part 3: Network Analysis of Movie Relationships

We can visualize the high-lift association rules as a network, where movies are nodes and a rule represents an edge between them.

## 3.1 Build and Visualize the Movie Network

We'll use the `igraph` package to create and plot the network.

```{r build-network}
# Extract movie pairs from the rules
pairs <- labels(subrules)
pairs <- gsub("\\{|\\}", "", pairs)
pairs <- strsplit(pairs, " => ")
pairs <- do.call(rbind, pairs)
pairs <- pairs[pairs[, 1] != "", ] # Remove rules with no left-hand side

# Create the graph
library(igraph)
movienet <- graph.edgelist(pairs, directed = FALSE)
movienet <- simplify(movienet, remove.multiple = TRUE) # Remove duplicate edges

summary(movienet)
```

Now, let's create a nice plot. We can make the size of each node proportional to its number of connections (**degree**).

```{r plot-network}
# You can customize this plot by changing the parameters
# Try different layouts like 'layout_with_fr' or 'layout_in_circle'
plot(movienet,
     vertex.label = NA, # Hiding labels for clarity
     edge.width = 0.5,
     vertex.size = degree(movienet) / 2, # Size nodes by degree
     vertex.color = "lightblue",
     layout = layout_with_kk(movienet))
```

## 3.2 Identify Key Movies with Centrality Measures

We can identify the most important movies in the network using centrality measures.
* **Degree**: The number of connections a node has (a measure of popularity).
* **Betweenness**: How often a node lies on the shortest path between two other nodes (a measure of being a "bridge").

```{r centrality}
# Top 10 movies by Degree
D <- degree(movienet)
o <- order(D, decreasing = TRUE)
V(movienet)$name[o[1:10]]

# Top 10 movies by Betweenness
B <- betweenness(movienet)
o <- order(B, decreasing = TRUE)
V(movienet)$name[o[1:10]]
```

***

# Part 4: Discovering Movie Communities

**Community detection** algorithms find groups of nodes (movies) that are more densely connected to each other than to the rest of the network. These communities might represent specific genres or thematic collections.

## 4.1 Find Communities with Edge Betweenness

The edge-betweenness algorithm works by progressively removing edges that have the highest betweenness centrality.

```{r community-detection}
cl <- cluster_edge_betweenness(movienet)

# We can find the optimal number of communities by looking at modularity.
# Modularity measures the strength of division of a network into communities.
plot(cl$modularity, type = "b", pch = 19, xlab = "Number of Communities",
     main = "Modularity Score")
abline(v = which.max(cl$modularity), lty = 3, lwd = 2, col = "red")

# Find the optimal number of communities based on the plot
best_cut <- which.max(cl$modularity)
cat("Optimal number of communities:", best_cut, "\n")

# Cut the dendrogram to get the community memberships
membership <- cut_at(cl, steps = best_cut - 1)
table(membership)
```

## 4.2 Visualize and Explore Communities

Let's plot the network again, this time coloring the nodes by their community membership.

```{r plot-communities}
plot(movienet,
     vertex.label = NA,
     vertex.size = 3,
     edge.curved = FALSE,
     vertex.color = membership)
```

We can now "zoom in" on a specific community to see what movies it contains. Let's look at community #1.

```{r inspect-community1}
# Get the subgraph for the first community
community1 <- induced_subgraph(movienet, vids = V(movienet)[membership == 1])

# Plot just this community
plot(community1,
     vertex.label.cex = 0.7,
     vertex.size = 5,
     edge.curved = FALSE)

# See which movies are in this community
V(community1)$name
```

***

# Part 5: Neighborhood and Ego Networks

Finally, we can explore the immediate neighborhood of a specific movie of interest (an "ego" node).

```{r ego-networks}
# This graph includes Vertigo and all of its direct neighbors
neighbor_Vertigo <- make_ego_graph(movienet, order = 1,
                                   nodes = "Vertigo (1958)",
                                   mode = "all")[[1]]
plot(neighbor_Vertigo, main = "Neighborhood of Vertigo")


# This graph includes Manhattan and all of its direct neighbors
neighbor_Manhattan <- make_ego_graph(movienet, order = 1,
                                     nodes = "Manhattan (1979)",
                                     mode = "all")[[1]]
plot(neighbor_Manhattan, main = "Neighborhood of Manhattan")

# Now, let's create the ego network for Manhattan by removing the central node.
# This shows us how Manhattan's neighbors are connected to each other.
ego1 <- delete_vertices(neighbor_Manhattan, "Manhattan (1979)")
plot(ego1, main = "Ego Network of Manhattan")
```

We can even run algorithms like **PageRank** on this small ego network to see which of Manhattan's neighbors are most central within that local context.

```{r pagerank-ego}
search <- page_rank(ego1)
V(ego1)$name[order(search$vector, decreasing = TRUE)]
```